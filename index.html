<!DOCTYPE html>
<html lang="en">
<head>
<meta charset="UTF-8" />
<meta name="viewport" content="width=device-width, initial-scale=1.0" />
<title>CS180 Project 4 — Neural Radiance Fields</title>

<style>
    body {
        margin: 0;
        font-family: Arial, sans-serif;
        background-color: #0e0e0e;
        color: #e6e6e6;
        line-height: 1.6;
    }

    /* Sticky Navbar */
    nav {
        position: sticky;
        top: 0;
        background: #111;
        padding: 15px;
        z-index: 1000;
        border-bottom: 1px solid #333;
    }

    nav a {
        color: #e6e6e6;
        margin-right: 20px;
        text-decoration: none;
        font-weight: bold;
    }

    nav a:hover {
        color: #4da3ff;
    }

    section {
        max-width: 900px;
        margin: auto;
        padding: 60px 20px;
        border-bottom: 1px solid #222;
    }

    h1, h2, h3 {
        color: #4da3ff;
    }

    .image-container {
        width: 100%;
        height: 300px;
        border: 1px dashed #555;
        margin: 20px 0;
        display: flex;
        align-items: center;
        justify-content: center;
        color: #777;
        font-size: 14px;
        background: #1a1a1a;
    }
</style>
</head>

<body>

<nav>
    <a href="#intro">Home</a>
    <a href="#part0">Part 0</a>
    <a href="#part1">Part 1</a>
    <a href="#part2">Part 2</a>
</nav>

<!-- INTRO -->
<section id="intro">
    <h1>CS180 Project 4 — Neural Radiance Fields</h1>

    <p>
        This project explores Neural Radiance Fields (NeRF) by implementing the pipeline from scratch. 
        Starting with camera calibration and 3D reconstruction, I built up to coordinate-based neural networks, 
        volumetric rendering, and multi-view reconstruction.  
        I tested the approach on standard datasets and my own captured scene.
    </p>
</section>

<!-- PART 0 -->
<section id="part0">
    <h1>Part 0 — Camera Calibration & 3D Capture</h1>

    <h2>0.1 Camera Calibration</h2>
    <div class="image-container">INSERT CHECKERBOARD IMAGES HERE</div>

    <p>
        I calibrated my camera using a printed ArUco checkerboard. By capturing images from multiple angles, 
        I estimated the intrinsic matrix (fx, fy, cx, cy) and distortion coefficients.  
        These parameters are essential for reconstructing accurate camera rays in later sections.
    </p>

    <div class="image-container">INSERT INTRINSICS.JSON SCREENSHOT HERE</div>


    <h2>0.2 Capturing a 3D Object Scene</h2>
    <div class="image-container">INSERT SAMPLE SCENE IMAGES HERE</div>

    <p>
        I captured a 360° video around an object with stable lighting and minimal motion blur.  
        Consistent parallax across frames is critical for successful structure-from-motion in COLMAP.
    </p>


    <h2>0.3 COLMAP Pose Estimation</h2>
    <div class="image-container">INSERT COLMAP SPARSE CLOUD HERE</div>
    <div class="image-container">INSERT CAMERA FRUSTUM VISUALIZATION HERE</div>

    <p>
        Using COLMAP’s feature extractor, matcher, and bundle adjustment steps, I reconstructed a sparse point cloud.  
        This yields the camera extrinsic matrices (rotation + translation) for each image, defining how the scene is viewed.
    </p>


    <h2>0.4 Dataset Undistortion</h2>
    <div class="image-container">INSERT ORIGINAL VS UNDISTORTED IMAGE HERE</div>

    <p>
        After reconstruction, I exported undistorted images and generated a <code>transforms.json</code> 
        compatible with NeRF-style rendering pipelines. This cleans the dataset and ensures geometric consistency.
    </p>

    <div class="image-container">INSERT DATASET FOLDER SCREENSHOT HERE</div>
</section>


<!-- PART 1 -->
<section id="part1">
    <h1>Part 1 — 2D Neural Fields</h1>

    <h2>1.1 Model Architecture</h2>
    <div class="image-container">INSERT MLP DIAGRAM OR HYPERPARAMETER TABLE HERE</div>

    <p>
        I trained a coordinate-based MLP that maps 2D coordinates to RGB values.  
        With positional encoding, the model learns high-frequency details and reconstructs an entire image 
        as a continuous mathematical function.
    </p>


    <h2>1.2 Reconstruction Results</h2>

    <h3>Fox Scene</h3>
    <div class="image-container">INSERT ORIGINAL FOX IMAGE</div>
    <div class="image-container">INSERT RECONSTRUCTED FOX IMAGE</div>

    <h3>Golden Gate</h3>
    <div class="image-container">INSERT ORIGINAL GOLDEN GATE IMAGE</div>
    <div class="image-container">INSERT RECONSTRUCTED GOLDEN GATE IMAGE</div>

    <p>
        The model learns coarse shapes first and refines them progressively, eventually matching the ground truth almost perfectly.
    </p>


    <h2>1.3 Training Progress</h2>
    <div class="image-container">INSERT INTERMEDIATE FOX OUTPUTS</div>
    <div class="image-container">INSERT INTERMEDIATE GOLDEN GATE OUTPUTS</div>

    <p>
        Early iterations produce blurry reconstructions, which gradually sharpen as the positional encoding layers 
        inject high-frequency features into the MLP.
    </p>


    <h2>1.4 Positional Encoding Sweep</h2>
    <div class="image-container">INSERT FREQUENCY SWEEP COMPARISON</div>
    <div class="image-container">INSERT WIDTH SWEEP COMPARISON</div>

    <p>
        Increasing the number of frequency bands improves detail but risks overfitting.  
        Wider networks generally improve PSNR but with diminishing returns.
    </p>


    <h2>1.5 PSNR Curves</h2>
    <div class="image-container">INSERT FOX PSNR CURVE</div>
    <div class="image-container">INSERT GOLDEN GATE PSNR CURVE</div>

    <p>
        The model rapidly increases in accuracy during early iterations before plateauing near convergence.
    </p>

</section>


<!-- PART 2 -->
<section id="part2">
    <h1>Part 2 — 3D Neural Radiance Field</h1>

    <h2>2.1 Ray Generation</h2>
    <div class="image-container">INSERT RAY VISUALIZATION HERE</div>

    <p>
        I built utilities to convert pixel coordinates into 3D rays using camera intrinsics and extrinsics.  
        Each ray originates at the camera center and passes through a pixel, forming the basis of volumetric rendering.
    </p>


    <h2>2.2 Sampling Along Rays</h2>
    <div class="image-container">INSERT SAMPLING DIAGRAM HERE</div>

    <p>
        I drew 64 sample points uniformly or with stratified jitter along each ray.  
        These points are inputs to the NeRF MLP, where density and color are predicted.
    </p>


    <h2>2.3 Dataset Loading</h2>
    <div class="image-container">INSERT RAY-BATCH DIAGRAM HERE</div>

    <p>
        Rays, sample points, and ground-truth RGB values were precomputed and cached for efficient training.  
        This removes redundant per-iteration computation.
    </p>


    <h2>2.4 Neural Field Model</h2>
    <div class="image-container">INSERT NERF MLP DIAGRAM HERE</div>

    <p>
        The NeRF network predicts both density and color for each 3D point.  
        I used an 8-layer architecture with positional encoding and a skip connection, following the original paper.
    </p>


    <h2>2.5 Volume Rendering</h2>
    <div class="image-container">INSERT VOLUME RENDERING FORMULA DIAGRAM HERE</div>

    <p>
        Using the classical volume rendering equation, I accumulated alpha and color contributions along each ray.  
        This creates realistic occlusion and soft transparency artifacts characteristic of NeRF.
    </p>


    <h2>2.6 Final: Training My NeRF Dataset</h2>

    <div class="image-container">INSERT 2–3 DATASET IMAGES HERE</div>
    <div class="image-container">INSERT TRANSFORMS.JSON SCREENSHOT HERE</div>

    <p>
        I attempted to train a NeRF with Nerfstudio using my captured dataset.  
        Although the data preparation pipeline succeeded, I encountered compatibility issues on macOS 
        (COLMAP GPU flags and ffmpeg).  
        Due to environment limitations, I was unable to complete full NeRF training.  
        However, I include:
        <ul>
            <li>All captured images</li>
            <li>The generated <code>transforms.json</code></li>
            <li>The prepared dataset folder</li>
        </ul>
    </p>

</section>

</body>
</html>
