<!DOCTYPE html>
<html lang="en">
<head>
  <meta charset="UTF-8" />
  <title>CS180 Project 4 – Neural Radiance Fields</title>
  <meta name="viewport" content="width=device-width, initial-scale=1" />
  <style>
    :root {
      --bg: #0f172a;
      --bg-alt: #020617;
      --card: #0b1220;
      --border: #1e293b;
      --accent: #38bdf8;
      --accent-soft: rgba(56, 189, 248, 0.15);
      --text: #e5e7eb;
      --muted: #9ca3af;
      --heading: #f9fafb;
      --radius-lg: 18px;
      --radius-sm: 10px;
      --shadow-soft: 0 18px 40px rgba(15, 23, 42, 0.6);
    }

    * {
      box-sizing: border-box;
    }

    body {
      margin: 0;
      font-family: system-ui, -apple-system, BlinkMacSystemFont, "SF Pro Text",
        "Segoe UI", sans-serif;
      background: radial-gradient(circle at top, #1f2937 0, #020617 55%, #000 100%);
      color: var(--text);
      -webkit-font-smoothing: antialiased;
    }

    a {
      color: var(--accent);
      text-decoration: none;
    }
    a:hover {
      text-decoration: underline;
    }

    main {
      max-width: 1120px;
      margin: 0 auto;
      padding: 32px 16px 64px;
    }

    header {
      text-align: center;
      margin-bottom: 32px;
    }
    header h1 {
      font-size: clamp(2.2rem, 3vw, 2.7rem);
      color: var(--heading);
      margin-bottom: 8px;
    }
    header p {
      margin: 4px 0;
      color: var(--muted);
    }

    .tagline {
      display: inline-flex;
      align-items: center;
      gap: 8px;
      padding: 6px 12px;
      border-radius: 999px;
      border: 1px solid var(--accent-soft);
      background: rgba(15, 23, 42, 0.9);
      color: var(--accent);
      font-size: 0.8rem;
      margin-bottom: 12px;
    }
    .tagline span {
      width: 7px;
      height: 7px;
      border-radius: 999px;
      background: #22c55e;
      box-shadow: 0 0 0 6px rgba(34, 197, 94, 0.15);
    }

    section {
      margin-bottom: 40px;
    }

    .section-header {
      margin-bottom: 18px;
    }
    .section-header h2 {
      font-size: 1.7rem;
      margin: 0 0 4px;
      color: var(--heading);
    }
    .section-header h3 {
      font-size: 1.2rem;
      margin: 0 0 6px;
      color: var(--heading);
    }
    .section-header p {
      margin: 0;
      color: var(--muted);
      max-width: 640px;
    }

    .card {
      background: linear-gradient(145deg, #020617 0%, #020617 55%, #020617 100%);
      border-radius: var(--radius-lg);
      border: 1px solid var(--border);
      padding: 18px 18px 18px;
      box-shadow: var(--shadow-soft);
    }

    .card-subtle {
      background: radial-gradient(circle at top left, #020617 0, #020617 40%, #020617 100%);
      border-radius: var(--radius-lg);
      border: 1px solid rgba(148, 163, 184, 0.12);
      padding: 16px 18px;
    }

    .two-column {
      display: grid;
      grid-template-columns: minmax(0, 1.1fr) minmax(0, 1fr);
      gap: 20px;
    }

    .pill-heading {
      font-size: 0.8rem;
      text-transform: uppercase;
      letter-spacing: 0.12em;
      color: var(--muted);
      margin: 0 0 6px;
    }

    ul {
      margin: 0 0 6px 18px;
      padding: 0;
      color: var(--muted);
      font-size: 0.95rem;
    }

    .image-row {
      display: flex;
      flex-wrap: wrap;
      gap: 16px;
      margin-top: 10px;
    }

    .image-col {
      flex: 1 1 260px;
      min-width: 220px;
    }

    figure {
      margin: 0;
    }
    figure img {
      width: 100%;
      border-radius: var(--radius-sm);
      border: 1px solid rgba(148, 163, 184, 0.28);
      display: block;
    }
    figcaption {
      margin-top: 6px;
      font-size: 0.85rem;
      color: var(--muted);
    }

    /* Equal height for reference/custom images */
    .card .image-row img {
      height: 100%;
      max-height: 330px;
      object-fit: cover;
    }

    .psnr-container {
      display: grid;
      grid-template-columns: minmax(0, 1fr);
      gap: 14px;
    }

    .stack {
      display: grid;
      gap: 14px;
    }

    nav {
      margin: 12px auto 28px;
      max-width: 720px;
      padding: 10px 14px;
      border-radius: 999px;
      background: rgba(15, 23, 42, 0.85);
      border: 1px solid rgba(148, 163, 184, 0.25);
      backdrop-filter: blur(18px);
    }
    nav ul {
      display: flex;
      list-style: none;
      justify-content: center;
      flex-wrap: wrap;
      gap: 12px;
      margin: 0;
      padding: 0;
      font-size: 0.86rem;
    }
    nav a {
      padding: 4px 10px;
      border-radius: 999px;
      border: 1px solid transparent;
      color: var(--muted);
    }
    nav a:hover {
      border-color: rgba(148, 163, 184, 0.55);
      text-decoration: none;
      color: #e5e7eb;
    }

    @media (max-width: 768px) {
      .two-column {
        grid-template-columns: minmax(0, 1fr);
      }
      nav {
        border-radius: 16px;
      }
      header {
        text-align: left;
      }
    }
  </style>
</head>
<body>
<main>

    <header>
      <div class="tagline"></div>
      <h1>NeRF from 2D Images & Multi-view Lego</h1>
      <p>Fitting coordinate-based neural networks to single images and a multi-view Lego scene.</p>
      <p>All results and plots below are produced by my implementation.</p>
    </header>

    <nav>
      <ul>
        <li><a href="#part0">Part 0 – Setup</a></li>
        <li><a href="#part1">Part 1 – 2D Neural Field</a></li>
        <li><a href="#part2">Part 2 – Lego NeRF</a></li>
      </ul>
    </nav>

    <!-- ===================== PART 0 ===================== -->
    <section id="part0">
      <div class="section-header">
        <h2>Part 0 – Calibrating Your Camera and Capturing a 3D Scan</h2>
        <p>
          This section summarizes how I captured my own images, estimated camera parameters,
          and validated that the calibrated frustums were consistent in 3D.
        </p>
      </div>

      <div class="card stack">
        <div class="stack">
          <h3>0.0 – Overview</h3>
          <p>
            I captured a set of photos of my scene, detected a Charuco board in each image, and
            used OpenCV to recover camera intrinsics and per-image poses. I then verified the
            calibration visually by rendering the camera frustums and sample rays in a shared
            world coordinate frame.
          </p>

          <div class="image-row">
            <div class="image-col">
              <figure>
                <img src="image/Viserperso1.png" />
                <figcaption>Camera frustum visualization – view 1.</figcaption>
              </figure>
            </div>
            <div class="image-col">
              <figure>
                <img src="image/Viserperso2.png" />
                <figcaption>Camera frustum visualization – view 2.</figcaption>
              </figure>
            </div>
          </div>
        </div>

        <div class="stack">
          <h3>0.1 – Calibrating Your Camera</h3>
          <p>
            I began by detecting ArUco markers in each calibration image using OpenCV’s detector,
            discarding any images where markers were not detected. This established reliable
            2D–3D correspondences for calibration.
          </p>
        </div>

        <div class="stack">
          <h3>0.2 – Camera Calibration</h3>
          <p>
            Using the accumulated detections, I ran <code>calibrateCameraCharuco</code> to estimate
            focal length, principal point, and lens distortion coefficients. I also obtained
            per-image extrinsic matrices mapping camera coordinates to world space.
          </p>
        </div>

        <div class="stack">
          <h3>0.3 – Pose Estimation</h3>
          <p>
            For my object images, I used <code>solvePnP</code> to compute per-frame camera pose from
            ArUco detections, converted to camera-to-world matrices for later NeRF training.
          </p>
        </div>

        <div class="stack">
          <h3>0.4 – Undistortion & Dataset Export</h3>
          <p>
            I undistorted all images using <code>cv2.undistort</code>, cropped borders, updated the
            intrinsics accordingly, and packaged everything into a <code>.npz</code> file containing
            RGB images, c2w poses, and focal length.
          </p>
        </div>
      </div>
    </section>

    <!-- ===================== PART 1 ===================== -->
    <section id="part1">
      <div class="section-header">
        <h2>Part 1 – Fitting a Neural Field to a Single 2D Image</h2>
        <p>
          I implemented a coordinate-based 2D neural field that maps pixel coordinates to RGB
          values, helping build intuition before moving to 3D NeRFs.
        </p>
      </div>

      <div class="card two-column">
        <div>
          <p class="pill-heading">Model & Training Setup</p>
          <h3>MLP Architecture</h3>
          <p style="color: var(--muted);">
            The model uses sinusoidal positional encoding followed by a 4-layer MLP
            with width 256, outputting RGB through a sigmoid.
          </p>
          <ul>
            <li>Positional encoding L = 10</li>
            <li>Width = 256</li>
            <li>4 hidden layers (ReLU)</li>
            <li>Training steps = 2000</li>
            <li>Batch size = 10,000 pixels</li>
            <li>Loss = MSE, Metric = PSNR</li>
          </ul>
        </div>

        <div class="card-subtle">
          <p class="pill-heading">Input Images</p>
          <p>
            I trained the model on two scenes: the provided fox image, and a custom sunset
            skatepark photograph. Their contrast in texture and lighting makes them ideal
            for testing positional encoding and MLP capacity.
          </p>
        </div>
      </div>

      <section style="margin-top: 30px;">
        <div class="card">
          <div class="image-row">
            <div class="image-col">
              <figure>
                <img src="image/fox.jpg" />
                <figcaption>Fox – original reference image.</figcaption>
              </figure>
            </div>
            <div class="image-col">
              <figure>
                <img src="image/skate.png" />
                <figcaption>Skatepark – custom input image.</figcaption>
              </figure>
            </div>
          </div>
        </div>
      </section>

      <section style="margin-top: 32px;">
        <div class="section-header">
          <h3>Hyperparameter Sweep Results</h3>
        </div>

        <div class="card">
          <figure>
            <img src="part1_outputs/fox_hypergrid.png" />
            <figcaption>Fox – Hyperparameter sweep.</figcaption>
          </figure>
        </div>

        <div class="card" style="margin-top: 18px;">
          <figure>
            <img src="part1_outputs/skate_hypergrid.png" />
            <figcaption>Skatepark – Hyperparameter sweep.</figcaption>
          </figure>
        </div>
      </section>

      <section style="margin-top: 32px;">
        <div class="section-header">
          <h3>Training Curves</h3>
        </div>

        <div class="card">
          <figure>
            <img src="part1_outputs/fox_training_curves.png" />
            <figcaption>Fox – Loss and PSNR curves.</figcaption>
          </figure>
        </div>

        <div class="card" style="margin-top: 18px;">
          <figure>
            <img src="part1_outputs/skate_training_curves.png" />
            <figcaption>Skatepark – Loss and PSNR curves.</figcaption>
          </figure>
        </div>
      </section>
    </section>

    <!-- ===================== PART 2 ===================== -->
    <section id="part2">
      <div class="section-header">
        <h2>Part 2 – Fit a Neural Radiance Field from Multi-view Images</h2>
        <p>
          I implemented the full NeRF pipeline: ray generation, sampling, density/color
          prediction, and volume rendering on the Lego dataset.
        </p>
      </div>

      <div class="card stack">
        <div class="stack">
          <h3>2.1 – Create Rays from Cameras</h3>
          <p>
            Pixel centers were converted to 3D rays using intrinsics inversion and camera pose
            transforms.
          </p>
        </div>

        <div class="stack">
          <h3>2.2 – Sampling</h3>
          <p>
            Stratified depth sampling generated evenly spaced 3D sample points along each ray.
          </p>
        </div>

        <div class="stack">
          <h3>2.3 – Putting the Dataloading All Together</h3>
          <p>
            Rays and RGB targets were precomputed into a compact structure for fast training.
          </p>
        </div>

        <div class="stack">
          <h3>2.4 – Neural Radiance Field</h3>
          <p>
            NeRF uses high-frequency positional encoding, a skip-connected MLP, and separate
            density/color heads.
          </p>
        </div>

        <div class="stack">
          <h3>2.5 – Volume Rendering</h3>
          <p>
            Sigmas were converted into alpha values and composited using accumulated
            transmittance to produce final pixel colors.
          </p>
        </div>
      </div>

      <section style="margin-top: 32px;">
        <div class="section-header">
          <h3>Ray & Sample Visualizations</h3>
        </div>

        <div class="card">
          <div class="image-row">
            <div class="image-col">
              <figure>
                <img src="image/ViserLego1.png" />
                <figcaption>Rays & samples – view 1.</figcaption>
              </figure>
            </div>
            <div class="image-col">
              <figure>
                <img src="image/ViserLego2.png" />
                <figcaption>Rays & samples – view 2.</figcaption>
              </figure>
            </div>
          </div>
        </div>
      </section>

      <section style="margin-top: 32px;">
        <div class="section-header">
          <h3>Training Progression – Lego</h3>
        </div>

        <div class="card">
          <p class="pill-heading">Lego – Training Steps</p>
          <figure>
            <img src="image/Lego_Training_Steps.png" />
            <figcaption>Validation renders at different training iterations.</figcaption>
          </figure>
        </div>
      </section>

      <section style="margin-top: 32px;">
        <div class="section-header">
          <h3>Validation PSNR & Spherical Render</h3>
        </div>

        <div class="card two-column">
          <div>
            <figure>
              <img src="image/Lego_Validation_PSNR.png" />
              <figcaption>Validation PSNR over iterations.</figcaption>
            </figure>
          </div>
          <div>
            <figure>
              <img src="image/lego_orbit.gif" />
              <figcaption>Final spherical orbit around the Lego scene.</figcaption>
            </figure>
          </div>
        </div>
      </section>
    </section>

    <footer style="margin-top: 40px; padding-top: 16px; border-top: 1px solid rgba(148,163,184,0.25); font-size: 0.8rem; color: var(--muted);">
      <p>
        CS180 Project 4 – Neural Radiance Fields · Your Name Here
      </p>
    </footer>

</main>
</body>
</html>
