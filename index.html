<!DOCTYPE html>
<html lang="en">
<head>
  <meta charset="UTF-8" />
  <title>CS180 Project 4 – Neural Radiance Fields</title>
  <meta name="viewport" content="width=device-width, initial-scale=1" />
  <style>
    :root {
      --bg: #0f172a;
      --bg-alt: #020617;
      --card: #0b1220;
      --border: #1e293b;
      --accent: #38bdf8;
      --accent-soft: rgba(56, 189, 248, 0.15);
      --text: #e5e7eb;
      --muted: #9ca3af;
      --heading: #f9fafb;
      --radius-lg: 18px;
      --radius-sm: 10px;
      --shadow-soft: 0 18px 40px rgba(15, 23, 42, 0.6);
    }

    * {
      box-sizing: border-box;
    }

    body {
      margin: 0;
      font-family: system-ui, -apple-system, BlinkMacSystemFont, "SF Pro Text",
        "Segoe UI", sans-serif;
      background: radial-gradient(circle at top, #1f2937 0, #020617 55%, #000 100%);
      color: var(--text);
      -webkit-font-smoothing: antialiased;
    }

    a {
      color: var(--accent);
      text-decoration: none;
    }
    a:hover {
      text-decoration: underline;
    }

    main {
      max-width: 1120px;
      margin: 0 auto;
      padding: 32px 16px 64px;
    }

    header {
      text-align: center;
      margin-bottom: 32px;
    }
    header h1 {
      font-size: clamp(2.2rem, 3vw, 2.7rem);
      color: var(--heading);
      margin-bottom: 8px;
    }
    header p {
      margin: 4px 0;
      color: var(--muted);
    }

    .tagline {
      display: inline-flex;
      align-items: center;
      gap: 8px;
      padding: 6px 12px;
      border-radius: 999px;
      border: 1px solid var(--accent-soft);
      background: rgba(15, 23, 42, 0.9);
      color: var(--accent);
      font-size: 0.8rem;
      margin-bottom: 12px;
    }
    .tagline span {
      width: 7px;
      height: 7px;
      border-radius: 999px;
      background: #22c55e;
      box-shadow: 0 0 0 6px rgba(34, 197, 94, 0.15);
    }

    section {
      margin-bottom: 40px;
    }

    .section-header {
      margin-bottom: 18px;
    }
    .section-header h2 {
      font-size: 1.7rem;
      margin: 0 0 4px;
      color: var(--heading);
    }
    .section-header h3 {
      font-size: 1.2rem;
      margin: 0 0 6px;
      color: var(--heading);
    }
    .section-header p {
      margin: 0;
      color: var(--muted);
      max-width: 640px;
    }

    .card {
      background: linear-gradient(145deg, #020617 0%, #020617 55%, #020617 100%);
      border-radius: var(--radius-lg);
      border: 1px solid var(--border);
      padding: 18px 18px 18px;
      box-shadow: var(--shadow-soft);
    }

    .card-subtle {
      background: radial-gradient(circle at top left, #020617 0, #020617 40%, #020617 100%);
      border-radius: var(--radius-lg);
      border: 1px solid rgba(148, 163, 184, 0.12);
      padding: 16px 18px;
    }

    .two-column {
      display: grid;
      grid-template-columns: minmax(0, 1.1fr) minmax(0, 1fr);
      gap: 20px;
    }

    .pill-heading {
      font-size: 0.8rem;
      text-transform: uppercase;
      letter-spacing: 0.12em;
      color: var(--muted);
      margin: 0 0 6px;
    }

    ul {
      margin: 0 0 6px 18px;
      padding: 0;
      color: var(--muted);
      font-size: 0.95rem;
    }

    .image-row {
      display: flex;
      flex-wrap: wrap;
      gap: 16px;
      margin-top: 10px;
    }

    .image-col {
      flex: 1 1 260px;
      min-width: 220px;
    }

    figure {
      margin: 0;
    }
    figure img {
      width: 100%;
      border-radius: var(--radius-sm);
      border: 1px solid rgba(148, 163, 184, 0.28);
      display: block;
    }
    figcaption {
      margin-top: 6px;
      font-size: 0.85rem;
      color: var(--muted);
    }

    /* Equal height for reference/custom images */
    .card .image-row img {
      height: 100%;
      max-height: 330px;
      object-fit: cover;
    }

    .psnr-container {
      display: grid;
      grid-template-columns: minmax(0, 1fr);
      gap: 14px;
    }

    .stack {
      display: grid;
      gap: 14px;
    }

    nav {
      margin: 12px auto 28px;
      max-width: 720px;
      padding: 10px 14px;
      border-radius: 999px;
      background: rgba(15, 23, 42, 0.85);
      border: 1px solid rgba(148, 163, 184, 0.25);
      backdrop-filter: blur(18px);
    }
    nav ul {
      display: flex;
      list-style: none;
      justify-content: center;
      flex-wrap: wrap;
      gap: 12px;
      margin: 0;
      padding: 0;
      font-size: 0.86rem;
    }
    nav a {
      padding: 4px 10px;
      border-radius: 999px;
      border: 1px solid transparent;
      color: var(--muted);
    }
    nav a:hover {
      border-color: rgba(148, 163, 184, 0.55);
      text-decoration: none;
      color: #e5e7eb;
    }

    @media (max-width: 768px) {
      .two-column {
        grid-template-columns: minmax(0, 1fr);
      }
      nav {
        border-radius: 16px;
      }
      header {
        text-align: left;
      }
    }
  </style>
</head>
<body>
<main>

    <header>
      <div class="tagline"></div>
      <h1>NeRF from 2D Images & Multi-view Lego</h1>
      <p>Fitting coordinate-based neural networks to single images and a multi-view Lego scene.</p>
      <p>All results and plots below are produced by my implementation.</p>
    </header>

    <nav>
      <ul>
        <li><a href="#part0">Part 0 – Setup</a></li>
        <li><a href="#part1">Part 1 – 2D Neural Field</a></li>
        <li><a href="#part2">Part 2 – Lego NeRF</a></li>
      </ul>
    </nav>

    <!-- ===================== PART 0 ===================== -->
    <section id="part0">
      <div class="section-header">
        <h2>Part 0 – Calibrating Your Camera and Capturing a 3D Scan</h2>
        <p>
          This section summarizes how I captured my own images, estimated camera parameters,
          and validated that the calibrated frustums were consistent in 3D.
        </p>
      </div>

      <div class="card stack">
        <div class="stack">
          <h3>0.0 – Overview</h3>
          <p>
            I captured a set of photos of my scene, detected a Charuco board in each image, and
            used OpenCV to recover camera intrinsics and per-image poses. I then verified the
            calibration visually by rendering the camera frustums and sample rays in a shared
            world coordinate frame.
          </p>

          <div class="image-row">
            <div class="image-col">
              <figure>
                <img src="image/Viserperso1.png" />
                <figcaption>Camera frustum visualization – view 1.</figcaption>
              </figure>
            </div>
            <div class="image-col">
              <figure>
                <img src="image/Viserperso2.png" />
                <figcaption>Camera frustum visualization – view 2.</figcaption>
              </figure>
            </div>
          </div>
        </div>

        <div class="stack">
          <h3>0.1 – Calibrating Your Camera</h3>
          <p>
            I began by detecting ArUco markers in each of my calibration images using OpenCV’s built-in ArUco detector. For every image, I converted it to grayscale and applied the detector to extract the 2D pixel coordinates of the four corners of the printed marker. Since some frames contained motion blur or the marker was partially out of view, I discarded any images where no ArUco corners were detected to avoid corrupting the calibration process. This resulted in a reliable set of 2D corner positions that I paired with the known 3D coordinates of the marker corners on the printed tag. These 2D–3D correspondences form the foundation for estimating both intrinsic parameters and per-image camera poses later on. 
          </p>
        </div>

        <div class="stack">
          <h3>0.2 – Camera Calibration</h3>
          <p>
            Using all successfully detected ArUco markers, I performed full camera calibration with OpenCV’s cv2.calibrateCamera function. This routine takes as input the 2D pixel coordinates of the detected marker corners in each image along with the known 3D geometry of the physical tag. The function returns estimates of the intrinsic matrix (which includes focal length and principal point) and a set of lens distortion coefficients modeling radial and tangential distortion. In addition to intrinsics and distortion, the calibration step also produces a rotation vector and translation vector for every calibration image. These extrinsics describe the position and orientation of the camera relative to the marker coordinate system. Since the ArUco tag defines a stable world origin in this setup, these extrinsics allow us to later convert each camera viewpoint into a consistent world-space coordinate frame.
          </p>
        </div>

        <div class="stack">
          <h3>0.3 – Pose Estimation</h3>
          <p>
            After estimating intrinsics, I computed the camera pose for every image of my captured object using OpenCV’s cv2.solvePnP function. For each object frame, I again detected the same ArUco marker, recovered its 2D pixel corner coordinates, and paired them with the known 3D coordinates of the marker in world space. Given these correspondences and the intrinsics computed in the previous step, solvePnP returns a rotation and translation describing where the camera is positioned relative to the marker. I then converted these rotation/translation parameters into full 4×4 camera-to-world transformation matrices, which are the format expected by NeRF. Importantly, I did not apply any additional normalization, scaling, or re-centering to the scene—the world frame is defined directly by the physical ArUco marker, and all camera poses remain consistent within that coordinate system.
          </p>
        </div>

        <div class="stack">
          <h3>0.4 – Undistortion & Dataset Export</h3>
          <p>
            Before packaging the dataset for NeRF, I first undistorted each image using the calibrated intrinsics and distortion coefficients obtained earlier. I used OpenCV’s cv2.undistort along with cv2.getOptimalNewCameraMatrix to both correct lens distortion and automatically crop away invalid black borders introduced during undistortion. Because this cropping shifts the image origin, I also updated the intrinsic matrix’s principal point to reflect the new pixel coordinates. Once all images were undistorted and all camera poses were computed, I split the dataset into training, validation, and test subsets. Finally, I saved everything—undistorted RGB images, their corresponding camera-to-world matrices, and the focal length—into a single compressed .npz file. This file can be directly loaded by my NeRF implementation for training and novel-view rendering.
          </p>
        </div>
      </div>
    </section>

    <!-- ===================== PART 1 ===================== -->
    <section id="part1">
      <div class="section-header">
        <h2>Part 1 – Fitting a Neural Field to a Single 2D Image</h2>
        <p>
          I implemented a coordinate-based 2D neural field that maps pixel coordinates to RGB
          values, helping build intuition before moving to 3D NeRFs.
        </p>
      </div>

      <div class="card two-column">
        <div>
          <p class="pill-heading">Model &amp; Training Setup</p>
          <h3>MLP Architecture</h3>
          <p style="color: var(--muted);">
            My 2D neural field is a fully-connected MLP that takes in sinusoidally encoded
            (x, y) pixel coordinates and predicts normalized RGB colors. Before encoding,
            I normalize pixel coordinates by dividing x by the image width and y by the image
            height so they lie in the range [0, 1]. Similarly, input RGB values are normalized
            to [0, 1] to stabilize optimization.
          </p>
          <ul>
            <li>Input: 2D pixel coordinate with positional encoding (L = 10).</li>
            <li>Hidden layers: 4 fully-connected layers with ReLU activations.</li>
            <li>Hidden width: 256 channels per layer.</li>
            <li>Output layer: 3 channels with sigmoid to clamp colors to [0, 1].</li>
          </ul>
        </div>


        <div class="card-subtle">
          <p class="pill-heading">Input Images</p>
          <p>
            I trained the model on two scenes: the provided fox image, and a custom sunset
            skatepark photograph. Their contrast in texture and lighting makes them ideal
            for testing positional encoding and MLP capacity.
          </p>
        </div>
      </div>

      <section style="margin-top: 30px;">
        <div class="card">
          <div class="image-row">
            <div class="image-col">
              <figure>
                <img src="image/fox.jpg" />
                <figcaption>Fox – original reference image.</figcaption>
              </figure>
            </div>
            <div class="image-col">
              <figure>
                <img src="image/skate.png" />
                <figcaption>Skatepark – custom input image.</figcaption>
              </figure>
            </div>
          </div>
        </div>
      </section>

      <section style="margin-top: 32px;">
        <div class="section-header">
          <h3>Hyperparameter Sweep Results</h3>
        </div>

        <div class="card">
          <figure>
            <img src="image/fox_hypergrid.png" />
            <figcaption>Fox – Hyperparameter sweep.</figcaption>
          </figure>
        </div>

        <div class="card" style="margin-top: 18px;">
          <figure>
            <img src="image/skate_hypergrid.png" />
            <figcaption>Skatepark – Hyperparameter sweep.</figcaption>
          </figure>
        </div>
      </section>

      <section style="margin-top: 32px;">
        <div class="section-header">
          <h3>Training Curves</h3>
        </div>

        <div class="card">
          <figure>
            <img src="image/fox_training_curves.png" />
            <figcaption>Fox – Loss and PSNR curves.</figcaption>
          </figure>
        </div>

        <div class="card" style="margin-top: 18px;">
          <figure>
            <img src="image/skate_training_curves.png" />
            <figcaption>Skatepark – Loss and PSNR curves.</figcaption>
          </figure>
        </div>
      </section>
    </section>

    <!-- ===================== PART 2 ===================== -->
    <section id="part2">
      <div class="section-header">
        <h2>Part 2 – Fit a Neural Radiance Field from Multi-view Images</h2>
        <p>
          I implemented the full NeRF pipeline: ray generation, sampling, density/color
          prediction, and volume rendering on the Lego dataset.
        </p>
      </div>

      <div class="card stack">
        <div class="stack">
          <h3>2.1 – Create Rays from Cameras</h3>
          <p>
            In this section, I converted pixel coordinates from each Lego training image into 3D rays. Using the intrinsic focal length and each camera-to-world transformation matrix, I computed a ray origin at the camera center and a normalized direction for every pixel. I followed the pinhole camera model precisely and ensured that the coordinate convention matched the dataset (positive z forward). This step produces a dense grid of rays per image, which becomes the foundation for all subsequent sampling and rendering.
          </p>
        </div>

        <div class="stack">
          <h3>2.2 – Sampling</h3>
          <p>
            Once the rays were generated, I performed stratified depth sampling along each ray. For every training iteration, I sampled a fixed number of points between a near and far bound that cover the Lego object. Each sample point stores its 3D world coordinate and the ray direction. My implementation uses uniform depth spacing, which is simple and stable for this dataset. These sampled locations are the exact points where the NeRF MLP will predict density and color.
          </p>
        </div>

        <div class="stack">
          <h3>2.3 – Putting the Dataloading All Together</h3>
          <p>
            To speed up training, I precomputed all rays and all ground-truth RGB colors for every training image. These were flattened into a large dataset of millions of rays. During training, I randomly draw batches of rays instead of sampling full images at once. This mimics the original NeRF training loop and makes optimization much faster. The dataloader returns matched triplets: ray origin, ray direction, and target RGB.
          </p>
        </div>

        <div class="stack">
          <h3>2.4 – Neural Radiance Field</h3>
          <p>
            My NeRF model predicts both volume density and emitted color at every sampled 3D point. I used positional encoding to allow the network to capture high-frequency details, applying a high-frequency encoding for 3D positions and a lower-frequency one for ray directions. The network follows the classic NeRF structure: a deeper MLP for density + features, and a second branch that uses both features and the encoded viewing direction to output RGB values. A sigmoid is applied to keep colors in [0,1].
          </p>
        </div>

        <div class="stack">
          <h3>2.5 – Volume Rendering</h3>
          <p>
            Finally, I implemented the NeRF volume-rendering equation. For each ray, densities are converted into alpha values, cumulative transmittance is computed, and RGBs from all sample points are combined using the standard NeRF weighting formula. The resulting single color becomes the predicted pixel value. This is compared to ground-truth RGB using MSE loss during training. After training, I reused the exact renderer to generate validation views and a full spherical orbit GIF.
          </p>
        </div>
      </div>

      <section style="margin-top: 32px;">
        <div class="section-header">
          <h3>Ray & Sample Visualizations</h3>
        </div>

        <div class="card">
          <div class="image-row">
            <div class="image-col">
              <figure>
                <img src="image/ViserLego1.png" />
                <figcaption>Rays & samples – view 1.</figcaption>
              </figure>
            </div>
            <div class="image-col">
              <figure>
                <img src="image/ViserLego2.png" />
                <figcaption>Rays & samples – view 2.</figcaption>
              </figure>
            </div>
          </div>
        </div>
      </section>

      <section style="margin-top: 32px;">
        <div class="section-header">
          <h3>Training Progression – Lego</h3>
        </div>

        <div class="card">
          <p class="pill-heading">Lego – Training Steps</p>
          <figure>
            <img src="image/Lego_Training_Steps.png" />
            <figcaption>The training steps are blurrier than I expected. With more iterations or a larger model, the refinement would likely improve, but even so, the progression still shows the network gradually learning the scene.</figcaption>
          </figure>
        </div>
      </section>

      <section style="margin-top: 32px;">
        <div class="section-header">
          <h3>Validation PSNR & Spherical Render</h3>
        </div>

        <div class="card two-column">
          <div>
            <figure>
              <img src="image/lego_psnr_curves.png" />
              <figcaption>Validation PSNR over iterations.</figcaption>
            </figure>
          </div>
          <div>
            <figure>
              <img src="image/lego_orbit.gif" />
              <figcaption>Final spherical orbit around the Lego scene.</figcaption>
            </figure>
          </div>
        </div>
      </section>
    </section>

    <footer style="margin-top: 40px; padding-top: 16px; border-top: 1px solid rgba(148,163,184,0.25); font-size: 0.8rem; color: var(--muted);">
      <p>
        CS180 Project 4 – Neural Radiance Fields · Antoine Peylet
      </p>
    </footer>

</main>
</body>
</html>
